Frontend:
  served_model_name: nvidia/Llama-3_3-Nemotron-Super-49B-v1
  endpoint: dynamo.Processor.chat/completions
  port: 8000
 
Processor:
  model: nvidia/Llama-3_3-Nemotron-Super-49B-v1
  block-size: 64
  max-model-len: 2048
  router: kv
  trust-remote-code: True
 
Router:
  model: nvidia/Llama-3_3-Nemotron-Super-49B-v1
  min-workers: 1
 
VllmWorker:
  model: nvidia/Llama-3_3-Nemotron-Super-49B-v1
  kv-transfer-config: '{"kv_connector":"DynamoNixlConnector"}'
  block-size: 64
  trust-remote-code: True
  max-model-len: 2048
  max-num-seqs: 16
  remote-prefill: true
  conditional-disagg: true
  max-local-prefill-length: 10
  max-prefill-queue-size: 2
  gpu-memory-utilization: 0.95
  tensor-parallel-size: 8
  router: kv
  enable-prefix-caching: true
  ServiceArgs:
    workers: 1
    resources:
      gpu: 8
 
PrefillWorker:
  model: nvidia/Llama-3_3-Nemotron-Super-49B-v1
  trust-remote-code: True
  kv-transfer-config: '{"kv_connector":"DynamoNixlConnector"}'
  block-size: 64
  max-model-len: 2048
  max-num-seqs: 16
  gpu-memory-utilization: 0.95
  tensor-parallel-size: 8
  ServiceArgs:
    workers: 1
    resources:
      gpu: 8
